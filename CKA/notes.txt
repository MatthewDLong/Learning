CKA notes

General exam tips from somebody who passed:

Kubernetes docs by heart where to find what, I opened doc for each question except for ad-hoc questions as i had little practice but it was worth seeing.
Knowledge in Kubernetes, intermediate level
Kodecloud CKA labs, not all but cluster setup/networking/troubleshooting a must.

---


apt reference

apt search netcat
apt install netcat=1.218-4ubuntu1
apt install -s netcat # Simulate installation
apt-cache policy netcat # Show available versions
apt list --installed
apt list --upgradeable
apt list netcat
apt install netcat
apt remove netcat
apt autoremove # remove auto installed packages
apt-file search --regexp 'bin/ip$' # find the package name for a command e.g. ip

YAML lint
apt install yamllint
yamllint pod.yaml

kubectl commands

Mark Node as unschedulable
kubectl cordon node01

Mark Node as schedulable
kubectl uncordon node01

View Pod logs (if kube-apiserver and etcd server is available)
kubectl logs -f etcd-master

View Pod logs (if kube-apiserver or etcd server is unavailable)
docker logs -f etcd-master

Drain Pods from a Node, cordons it and marks the Node as unschedulable
kubectl drain node01

Upgrading kubeadm
apt upgrade -y kubeadm=1.12.0-00

Upgrading kubelet
apt upgrade -y kubelet=1.12.0-00
systemctl restart kubelet

Upgrade Node config of kubelet version
kubeadm upgrade node config --kubelet-version v1.12.0
systemctl restart kubelet

Upgrading cluster components
kubeadm upgrade plan
kubeadm upgrade apply v1.13.4

Backup config of all resources in a cluster using the kube-apiserver
kubectl get all --all-namespaces -o yaml > all-resources.yaml

Backup etcd
ETCDCTL_API=3 etcdctl \
snapshot save snapshot.db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/etcd/ca.crt \
  --cert=/etc/etcd/etcd-server.crt \
  --key=/etc/etcd/etcd-server.key


Restore a etcd backup

Stop the kube-apiserver because restoring etcd from a backup requires restarting etcd and kube-apiserver depends on etcd:

service kube-apiserver stop

ETCDCTL_API=3 etcdctl \
snapshot restore snapshot.db \
  --data-dir=/var/lib/etcd-restore
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/etcd/ca.crt \
  --cert=/etc/etcd/etcd-server.crt \
  --key=/etc/etcd/etcd-server.key

This a new data directory at /var/lib/etcd-restore

Update etcd.service file with a new --data-dir value of /var/lib/etcd-backup

Reload the service daemon
systemctl daemon-reload

Restart the etcd service
service etcd restart

Start the kube-apiserver service
service kube-apiserver start

etcd lab notes

ETCDCTL_API=3 etcdctl \
snapshot save /opt/snapshot-pre-boot.db \
--endpoints=https://127.0.0.1:2379 \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key

ETCDCTL_API=3 etcdctl \
snapshot restore /opt/snapshot-pre-boot.db \
--data-dir=/var/lib/etcd-restore-1 \
--endpoints=https://127.0.0.1:2379 \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key

Viewing service logs:
journalctl -u etcd.service -l

OpenSSL

There are client certificates and server certificates for various components, see below:

ï¿¼

Viewing a certificate:
openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout

Certificate Authority

Generate private key:
openssl genrsa -out ca.key 2048

Generate certificate signing request:
openssl req -new -key ca.key -subj "/CN=KUBERNETES-CA" -out ca.csr

Generate a signed certificate with the private key:
openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt

Client certificates

Admin User

Generate private key:
openssl genrsa -out admin.key 2048

Generate certificate signing request:
openssl req -new -key admin.key -subj "/CN=kube-admin/O=system:masters" -out admin.csr

Generate a signed certificate with the private key:
openssl x509 -req -in admin.csr -CA ca.crt -CAkey ca.key -out admin.crt

KUBE SCHEDULER

Generate private key:
openssl genrsa -out kube-scheduler.key 2048

Generate certificate signing request:
openssl req -new -key kube-scheduler.key -subj "/CN=SYSTEM:KUBE-SCHEDULER" -out kube-scheduler.csr

Generate a signed certificate with the private key:
openssl x509 -req -in kube-scheduler.csr -CA ca.crt -CAkey ca.key -out kube-scheduler.crt

KUBE-CONTROLLER-MANAGER

Generate private key:
openssl genrsa -out kube-controller-manager.key 2048

Generate certificate signing request:
openssl req -new -key kube-controller-manager.key -subj "/CN=SYSTEM:KUBE-CONTROLLER-MANAGER" -out kube-controller-manager.csr

Generate a signed certificate with the private key:
openssl x509 -req -in kube-controller-manager.csr -CA ca.crt -CAkey ca.key -out kube-controller-manager.crt

KUBE-PROXY

Generate private key:
openssl genrsa -out kube-proxy.key 2048

Generate certificate signing request:
openssl req -new -key kube-proxy.key -subj "/CN=KUBE-PROXY" -out kube-proxy.csr

Generate a signed certificate with the private key:
openssl x509 -req -in kube-proxy.csr -CA ca.crt -CAkey ca.key -out kube-proxy.crt

Using the Admin User certificate to authenticate and authorize to the kube-apiserver:

curl https://kube-apiserver:6443/api/v1/pods/ --key admin.key --cert admin.crt --cacert ca.crt

curl https://kube-apiserver:6443/version --key admin.key --cert admin.crt --cacert ca.crt

curl https://kube-apiserver:6443/api/v1/pods --key admin.key --cert admin.crt --cacert ca.crt

Start a proxy server which uses your current credentials and serves an api at 127.0.0.1:8001:

kubectl proxy

Then you can curl 127.0.0.1:8001 without specifying credentials.

curl https://127.0.0.1:8001/api/v1/pods

Server certificates

ETCD-SERVER

Generate private key:
openssl genrsa -out etcd-server.key 2048

Generate certificate signing request:
openssl req -new -key etcd-server.key -subj "/CN=ETCD-SERVER" -out etcd-server.csr

Generate a signed certificate with the private key:
openssl x509 -req -in etcd-server.csr -CA ca.crt -CAkey ca.key -out etcd-server.crt

ETCD-PEER

Generate private key:
openssl genrsa -out etcd-peer.key 2048

Generate certificate signing request:
openssl req -new -key etcd-peer.key -subj "/CN=ETCD-PEER" -out etcd-peer.csr

Generate a signed certificate with the private key:
openssl x509 -req -in etcd-peer.csr -CA ca.crt -CAkey ca.key -out etcd-peer.crt

KUBE API SERVER

Generate private key:
openssl genrsa -out apiserver.key 2048

Generate certificate signing request:
# openssl.cnf
[req]
req_extensions = v3_req
distinguished_name = v3_distinguished_name
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation
subjectAltName = @alt_names
[alt_names]
DNS.1 = kubernetes
DNS.2 = kubernetes.default
DNS.3 = kubernetes.default.svc
DNS.4 = kubernetes.default.svc.cluster.local
IP.1 = 10.96.0.1
IP.2 = 172.17.0.87

openssl req -new -key apiserver.key -subj "/CN=kube-apiserver" -out apiserver.csr -config openssl.cnf

Generate a signed certificate with the private key:
openssl x509 -req -in apiserver.csr -CA ca.crt -CAKey ca.key -out apiserver.crt

KUBECTL NODES (SERVER)

node01

Generate private key:
openssl genrsa -out node01.key 2048

Generate certificate signing request:
openssl req -new -key node01.key -subj "/CN=node01" -out node01.csr

Generate a signed certificate with the private key:
openssl x509 -req -in node01.csr -CA ca.crt -CAkey ca.key -out node01.crt

node02

Generate private key:
openssl genrsa -out node02.key 2048

Generate certificate signing request:
openssl req -new -key node02.key -subj "/CN=node02" -out node02.csr

Generate a signed certificate with the private key:
openssl x509 -req -in node02.csr -CA ca.crt -CAkey ca.key -out node02.crt

node03

Generate private key:
openssl genrsa -out node03.key 2048

Generate certificate signing request:
openssl req -new -key node03.key -subj "/CN=node03" -out node03.csr

Generate a signed certificate with the private key:
openssl x509 -req -in node03.csr -CA ca.crt -CAkey ca.key -out node03.crt

If using kubeadm to manage certs, config file is stored at: /etc/kubernetes/manifests/kube-apiserver.yaml

Adding a new user:

The user must generate a private key:
openssl genrsa -out matt.key 2048

e.g.

-----BEGIN RSA PRIVATE KEY-----
MIIEpAIBAAKCAQEA7/euyeViIRdS1JXSphORTwIZwUrUE+fsTzl283GEeTE+cnkp
oWdmDKMD+AiUwXmNSe4bZQv3q+ikLkN37GcSw5nkn40WQJcnlP3+TASQvMst+uZ6
Iktg/5uKtPwnI4HuDaiuCapxzsJfYY/LTq1JI9fUlF0E507INeDHHD5sb1Yz5N0m
YWHkUTOeUc7SwLBcDMGVCr/5cBTNRhDFyCOJJTuEWMU4LoHhBGnst/CgxK0K4qcF
WL98wKi0sLnF3HDo9aTa2A7edJW7oyaZWiSEZtLa1XFVPFXT7UaAe6l1uUKAFxFK
S6VaiRBhGDDFBPpkjjfOQLZx4P2LoN1l2IP99QIDAQABAoIBAFSqjBYaA8RvKT9u
WTNVhhhdQl8LmsYnNx5AxTJho/7qnADhtQmIpN3iilzyEiItU+d4xluhnFIgNAVK
sr5LI7i1zI9qxSYm7sVH1HhXyAWEnoV+2irNc7zb0VXH/bsudaKGGFm6tIrh06tl
g7MPutWVqiBqrXc7ObKgyz+w2qtxDNF0XBqv/LjOy5W7vWY1vljetrZyUZiPZ414
307OSSLQL0U2ustnm5c0ULXrti+oq9MnQ3u58AgwYChVGlTIpZxs1Pd6lQAQSU7v
cSpc7WIjc964pozgQBETDk2otTWLelO6xzsKCk85i+OIyy/5wooJVg2R493on+SC
Ww4g2KECgYEA/K1I7kOPAKTaYVxwBZa6Sppx0ZcinguCkHINtxVb8ucRWIbq1zuL
MATeR8njPstdpxmqLZXLLmT0ToBcGsWy8vPIoaHxtIeJEyvfjowA4T+B7chC3g1C
SIJ5eXs88CwACMBuWdfeVzREzC1HnwgY8eEuv4aIx/5oT8NfaYSWqykCgYEA8x+b
mPeSWcLfcG8PyNuWHmSgejPdvy3eSN0smV56C25BLR8p1jmgd/6bAED7b2CEhr0p
oUThr9xyjgPvJeGOXIPU4QDjcbRlEZtfWn+CpG/moYs16dqVO8zm1If1L7a5j49a
bFSK3sg8jR9hfBG0yr/ySHQkdWYMX5+wlgNlYe0CgYBd1bRp12j54j6/PWbUjH8u
SWDPZwMSElAweEqjnSkCcdE9QQ1hVNmBwa0Sl+TuYvOqpJtQRnqoBhG8PWr2/khQ
dTwi2lM6qcK7/eUNuOxyufT0axR6Bg6BgwK9nIiq4Iw6s8vHTaViWSTbdk1Mv+MT
oH6N3dZZ/x6Z0IJUW0rg0QKBgQDTe7st5K05x3n1o6ZnYhvDdGTj6BoMIyoo17vj
7XrcVpv7VBBgeOBNgPvzzJLq31pomkOMNQ7cmzZKssFkgkAD2eljkBJI5lLEbhMT
9bb+sxg7jYzoT2thCWwBlAKD7r6bUmeltCTYhfWmDJYQBpbthuqKe/z64joBNDe3
Rlz6QQKBgQDrO8RdHbSkMaom4TQ96qJjSyOgZSFno7dWt4Odf94C9hw0uMZZEjka
bKzyR84EX16DG9gwX8MjckXPYF8OpPAGZDHAy92ejNye9cbMEZWZVC5gAtcZ6CPZ
WfW5iPF/3Ok64Iaqc9YldZViZXkzXyVDbuOZmb5Eg5h7JdSqLzfiSQ==
-----END RSA PRIVATE KEY-----

The user must generate a certificate signing request:
openssl req -new -key matt.key -subj "/CN=matt" -out matt.csr

e.g.

-----BEGIN CERTIFICATE REQUEST-----
MIICVDCCATwCAQAwDzENMAsGA1UEAwwEbWF0dDCCASIwDQYJKoZIhvcNAQEBBQAD
ggEPADCCAQoCggEBAO/3rsnlYiEXUtSV0qYTkU8CGcFK1BPn7E85dvNxhHkxPnJ5
KaFnZgyjA/gIlMF5jUnuG2UL96vopC5Dd+xnEsOZ5J+NFkCXJ5T9/kwEkLzLLfrm
eiJLYP+birT8JyOB7g2orgmqcc7CX2GPy06tSSPX1JRdBOdOyDXgxxw+bG9WM+Td
JmFh5FEznlHO0sCwXAzBlQq/+XAUzUYQxcgjiSU7hFjFOC6B4QRp7LfwoMStCuKn
BVi/fMCotLC5xdxw6PWk2tgO3nSVu6MmmVokhGbS2tVxVTxV0+1GgHupdblCgBcR
SkulWokQYRgwxQT6ZI43zkC2ceD9i6DdZdiD/fUCAwEAAaAAMA0GCSqGSIb3DQEB
CwUAA4IBAQDuwZcw5mcJlM5q/S2pzwGJVfBfxLrX9hLK1uyTVEbTeIvC626NRQfN
2WAfLuvPkFR6pDyDbKmv915Y5d+milmQiT1YnkNK1i+Ru22n/FyFojcwrpSW5laj
N9uWjIfOkCY+a3a7StN3mr8yNKrlOZriopfIKGhL05fNn9f+OSAkMmzdC/TggBid
uWeocyn+Mbr23DBtxtIITvX8qizH03TCIr7aznnQOxLkbRgYXPbQ+f/Holi7+gBS
+z82S180EKVsl2PXfH39G7G1rOfOrb5BEseV6dYayx4sidZziRChDeee6jEkGqMr
02nGd8zCgLG0YSQ2bbr3MTPnCSbcZ3ne
-----END CERTIFICATE REQUEST-----

The user must encode the CSR in base64:
cat matt.csr | base64 | tr -d "\n"

e.g.

LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1ZEQ0NBVHdDQVFBd0R6RU5NQXNHQTFVRUF3d0ViV0YwZERDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRApnZ0VQQURDQ0FRb0NnZ0VCQU8vM3JzbmxZaUVYVXRTVjBxWVRrVThDR2NGSzFCUG43RTg1ZHZOeGhIa3hQbko1CkthRm5aZ3lqQS9nSWxNRjVqVW51RzJVTDk2dm9wQzVEZCt4bkVzT1o1SitORmtDWEo1VDkva3dFa0x6TExmcm0KZWlKTFlQK2JpclQ4SnlPQjdnMm9yZ21xY2M3Q1gyR1B5MDZ0U1NQWDFKUmRCT2RPeURYZ3h4dytiRzlXTStUZApKbUZoNUZFem5sSE8wc0N3WEF6QmxRcS8rWEFVelVZUXhjZ2ppU1U3aEZqRk9DNkI0UVJwN0xmd29NU3RDdUtuCkJWaS9mTUNvdExDNXhkeHc2UFdrMnRnTzNuU1Z1Nk1tbVZva2hHYlMydFZ4VlR4VjArMUdnSHVwZGJsQ2dCY1IKU2t1bFdva1FZUmd3eFFUNlpJNDN6a0MyY2VEOWk2RGRaZGlEL2ZVQ0F3RUFBYUFBTUEwR0NTcUdTSWIzRFFFQgpDd1VBQTRJQkFRRHV3WmN3NW1jSmxNNXEvUzJwendHSlZmQmZ4THJYOWhMSzF1eVRWRWJUZUl2QzYyNk5SUWZOCjJXQWZMdXZQa0ZSNnBEeURiS212OTE1WTVkK21pbG1RaVQxWW5rTksxaStSdTIybi9GeUZvamN3cnBTVzVsYWoKTjl1V2pJZk9rQ1krYTNhN1N0TjNtcjh5TktybE9acmlvcGZJS0doTDA1Zk5uOWYrT1NBa01temRDL1RnZ0JpZAp1V2VvY3luK01icjIzREJ0eHRJSVR2WDhxaXpIMDNUQ0lyN2F6bm5RT3hMa2JSZ1lYUGJRK2YvSG9saTcrZ0JTCit6ODJTMTgwRUtWc2wyUFhmSDM5RzdHMXJPZk9yYjVCRXNlVjZkWWF5eDRzaWRaemlSQ2hEZWVlNmpFa0dxTXIKMDJuR2Q4ekNnTEcwWVNRMmJicjNNVFBuQ1NiY1ozbmUKLS0tLS1FTkQgQ0VSVElGSUNBVEUgUkVRVUVTVC0tLS0tCg==

The user sends the base64 encoded CSR to the Admin of the kubernetes cluster

The Admin creates a CertificateSigningRequest object using the kube-apiserver via kubectl:

# matt-csr.yml
apiVersion: certificates.k8s.io/v1beta1
kind: CertificateSigningRequest
metadata:
  name: matt
spec:
  groups:
  - system:authenticated
  usages:
  - digital signature
  - key encipherment
  - server auth
  request:  LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1ZEQ0NBVHdDQVFBd0R6RU5NQXNHQTFVRUF3d0ViV0YwZERDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRApnZ0VQQURDQ0FRb0NnZ0VCQU8vM3JzbmxZaUVYVXRTVjBxWVRrVThDR2NGSzFCUG43RTg1ZHZOeGhIa3hQbko1CkthRm5aZ3lqQS9nSWxNRjVqVW51RzJVTDk2dm9wQzVEZCt4bkVzT1o1SitORmtDWEo1VDkva3dFa0x6TExmcm0KZWlKTFlQK2JpclQ4SnlPQjdnMm9yZ21xY2M3Q1gyR1B5MDZ0U1NQWDFKUmRCT2RPeURYZ3h4dytiRzlXTStUZApKbUZoNUZFem5sSE8wc0N3WEF6QmxRcS8rWEFVelVZUXhjZ2ppU1U3aEZqRk9DNkI0UVJwN0xmd29NU3RDdUtuCkJWaS9mTUNvdExDNXhkeHc2UFdrMnRnTzNuU1Z1Nk1tbVZva2hHYlMydFZ4VlR4VjArMUdnSHVwZGJsQ2dCY1IKU2t1bFdva1FZUmd3eFFUNlpJNDN6a0MyY2VEOWk2RGRaZGlEL2ZVQ0F3RUFBYUFBTUEwR0NTcUdTSWIzRFFFQgpDd1VBQTRJQkFRRHV3WmN3NW1jSmxNNXEvUzJwendHSlZmQmZ4THJYOWhMSzF1eVRWRWJUZUl2QzYyNk5SUWZOCjJXQWZMdXZQa0ZSNnBEeURiS212OTE1WTVkK21pbG1RaVQxWW5rTksxaStSdTIybi9GeUZvamN3cnBTVzVsYWoKTjl1V2pJZk9rQ1krYTNhN1N0TjNtcjh5TktybE9acmlvcGZJS0doTDA1Zk5uOWYrT1NBa01temRDL1RnZ0JpZAp1V2VvY3luK01icjIzREJ0eHRJSVR2WDhxaXpIMDNUQ0lyN2F6bm5RT3hMa2JSZ1lYUGJRK2YvSG9saTcrZ0JTCit6ODJTMTgwRUtWc2wyUFhmSDM5RzdHMXJPZk9yYjVCRXNlVjZkWWF5eDRzaWRaemlSQ2hEZWVlNmpFa0dxTXIKMDJuR2Q4ekNnTEcwWVNRMmJicjNNVFBuQ1NiY1ozbmUKLS0tLS1FTkQgQ0VSVElGSUNBVEUgUkVRVUVTVC0tLS0tCg==

The Admin views the CSR:
kubectl get csr

The Admin approves the CSR
kubectl certificate approve matt

Kubernetes will sign the certificate using the CA key pair, and generates a cert for the user.

The Admin extracts the certificate:

kubectl get csr matt -o yaml

The signed certificate is under the status.certificate key in base64 encoded format.

The Admin decodes the signed certificate:

echo "signed certificate content here" | base64 --decode

The Admin then shares this with the user, who can use this certificate to authenticate and authorise when send requests to the kube-apiserver.

Passing in certs to kubectl:

kubectl get pods --server https://my-server:6443 --client-key admin.key --client-certificate admin.crt --certificate-authority ca.crt

kube config is stored at $HOME/.kube/config

View current config:
kubectl config view

Change current context:
kubectl config use-context prod-user@production

Creating a Role and RoleBinding:

Create two files containing the Role and RoleBinding

kubectl create -f role.yml

kubectl create role-binding.yml

View roles: kubectl get roles

View role bindings: kubectl get rolebindings

Checking access:

kubectl auth can-i delete nodes

Impersonating another users permissions:

kubectl auth can-i create deployments --as dev-user --namespace red

Resources are either Namespaced or Cluster Scoped

View namespaced resources:

kubectl api-resources --namespaced=true
e.g.
pods
replicasets
jobs
deployments
services
secrets
roles
rolebindings
configmaps

View non namespaced resources:
kubectl api-resources --namespaced=false
e.g.
nodes
clusterroles
clusterrolebindings
certificatesigningrequests

Creating service accounts (machine users)
e.g.
kubectl create serviceaccount dashboard-sa

This creates a token as a Secret object

Get a secret:
e.g.
kubectl describe secret dashboard-sa-secret

There's a default service account with a token created

Pods are created with a secret volume which contains the token generated

In a pod definition file, specify: serviceAccountName:
to use the non default service account

Logging in to private Docker registry:

docker login private-registry.com
docker pull private-registry.com/nginx

Pass in credentials of the private image registry to the Docker runtime in worker nodes

Create a secret of type 'docker-registry' named regcred:

kubectl create secret docker-registry regcred --docker-server=private-registry.com --docker-username=foo --docker-password=bar --docker-email=foo@bar.com

In Pod definition file (in pod.spec) specify:

imagePullSecrets:
name: regcred

When Pod is created, kubelet on worker node, uses docker-registry secret to authenticate to private docker registry and pull images.

Container security context:

Run Docker container as given user:

docker run --user=1000 ubuntu sleep 3600

Add capability:

docker run --cap-add MAC_ADMIN ubuntu

Can configure security at a Pod level or a container level
Pod level - applied to all containers in the Pod
Container level, will override the settings on the Pod

In Pod definition file in spec (Pod level):

securityContext:
  runAsUser: 1000

Or alternatively under a container specification, to apply it to a container

At a continer level, you can also add:

securityContext:
  capabilities:
    add: ["MAC_ADMIN"]

Networking

By default, Kubernetes has an "all allow" rule which allows all pods in the cluster to communicate with each other Ingress and Egress.

Network Policys can specify rules for Ingress and Egress for Pods e.g. Allow Ingress 3306 on Pod db-pod from Pod api-pod

A NetworkPolicy is a Resource in kubernetes

NetworkPolicys are linked to Pods by Selectors

e.g.

In the NetworkPolicy defintion file:

podSelector:
  matchLabels:
    role: db

In the db-pod Pod definition file:

labels:
  role: db

NetworkPolicys are enforced by the networking solution that is implemented on the kubernetes cluster e.g.
kube-router
calico
romana
weave-net

If you specify an Ingress rule, you do not need to specify an Egress rule in order for the response to be sent back to the client which connects and queries using that Ingress rule.

Switching and Routing

Install ip command on Mac: brew install iproute2mac

List interfaces on a Host:
ip link

A Switch allows devices to communicate within a network.

ip addr: See ip addresses assigned to interfaces

Add a device ip address to an interface:

ip addr add 192.168.1.10/24 dev eth0

A Router is used to connect two devices on different networks. The Router is the Gateway.

Add an entry to the routing table:

ip route add 192.168.2.1/24 via 192.168.1.1

Set the Default Gateway on the routing table:

ip routeÂ addÂ default via 192.168.1.1

This means any request to an ip outside the local network will go via the Router.

View the routing table:

netstat -rn

io forwarding is decided by: /proc/sys/net/ipv4/ip_forward

Network namespaces

Network namespaces are used to implement network isolation.

View interfaces:

ip link

Create a network namespace (Not mac, but Linux):

ip netns add blue

View interfaces on a network namespace:

ip netns exec blue ip link

Or:

ip -n blue link

View the ARP table:

arp -a -n

View the ARP table inside a network namespace:

ip netns exec blue arp

Or

ip -n blue arp

Process namespaces are used by Docker, this is why: inside a container, running `ps aux` you see only one pid e.g. PID = 1, COMMAND = nginx
and on the underlying Host machine, you would see several processes, and the process that is running inside the Docker container has a different PID on the underlying host e.g. 3182

Because a Docker container runs inside its own network namespace, it has it's own: Route table, ARP table and Virtual Interfaces e.g. eth0. The underlying host also has a Route table, ARP table and Virtual Interfaces. To demonstrate this, complete the below:

When you first create a new network namespace (e.g. ip netns add blue), initially they have no network connectivity.

You can connect two network namespaces together using a "virtual ethernet cable" or "a pipe"

e.g. if you have two network namespaces red and blue, you can create a virtual ethernet cable to connect the two namespaces, by adding a virtual interface to each.

First, create two new virtual ethernet interfaces, once called veth-red and the second called veth-blue:

ip link add veth-red type veth peer name veth-blue

Next, attach the two new interfaces to the the namespaces:

ip link set veth-red netns red

ip link set veth-blue netns blue

Next, assign an ip address to each network namespace:

ip -n red addr add 192.168.15.1/24 dev veth-red

ip -n blue addr add 192.168.15.2/24 dev veth-blue

Next, verify that the IPv4 address was set on the blue and red network namespace:

ip netns exec blue ip addr list

Next, bring up the two new virtual ethernet interfaces:

ip -n red link set veth-red up

ip -n blue link set veth-blue up

Next, verify that the veth-red and veth-blue interfaces are up:

ip netns exec blue ip link

ip netns exec red ip link

Next, ping red from the blue network namespace:

ip netns exec blue ping 192.168.15.1

View the ARP table from the red network namespace:

ip netns exec red arp

You will see an entry pointing to the blue network namespace, and vice versa for the blue namespace

Next, view the ARP table on the host, you will see no reference to the veth-red or veth-blue vitual ethernet interfaces, but you will see the ARP table for the host.

Delete virtualt devices with:

ip -n red link del veth-red
ip -n blue link del veth-blue

Docker network options

none, host and bridge

e.g.

docker run --network none -d nginx # No network

docker run --network host -d nginx # Container attached to host network, no network isolation between the host and the container

docker run --network bridge -d nginx

For the bridge option, an internal private network is created, which the container is attached to.

The network has an ip of 172.17.0.0 by default, and each device connecting to the network get their own private ip on the network e.g. 172.17.0.2

When Docker is run on a host, it creates an internal private network calledbridgeby default.

View it with: docker network ls

Kubernetes Networking

Default ports for Master Node (Control Plane) and Worker Node

https://kubernetes.io/docs/reference/ports-and-protocols/

Main commands for inspecting the networking setup of a kubernetes cluster:

ip link # Display all interfaces

ip addr # Display IP Addresses

ip addr add 192.168.1.10/24 dev eth0 # Add an address to device eth0

ip route # Display routing table

ip route add 192.168.1.0/24 via 192.168.2.1 # Add a route to 192.168.1.0/24 via the gateway at 192.168.1.1

cat /proc/sys/net/ipv4/ip_forward # View IP forwarding setting

arp # View ARP table (Address Resolution Protocol â IP-address-to-MAC-addressÂ mappings)

netstat -plnt # View tcp sockets

netstat -anp # View all sockets

Installing a CNI plugin:
https://v1-22.docs.kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/#common-tasks-after-bootstrapping-control-plane

Kubernetes Pod Networking Model
- Every Pod must have an IP address
- Every Pod must be able to communicate with every other Pod in the same Node
- Every Pod must be able to communicate with every other Pod on other Nodes without NAT

CNI Weave

Inspect the kubelet service:
ps aux | grep kubelet

Inspect which --network-plugin is configured on kubelet:
ps aux | grep --regexp '--network-plugin'

View available CNI binaries:
ls /opt/cni/bin

Identify the CNI plugin configured to be used on a kubernetes cluster:
ls /etc/cni/net.d/

DeployingÂ weave-netÂ networking solution to the cluster: Replacing the default IP address and subnet ofÂ weave-netÂ to theÂ 10.50.0.0/16 â it defaults to 10.32.0.0/12

Get the host ip:
ip a | grep eth0
e.g. 10.64.203.8/24

If you deploy a weave manifest file directly without changing the default IP addresses it will overlap with the host system IP addresses and as a result, it's weave pods will go into anÂ ErrorÂ orÂ CrashLoopBackOffÂ state.

Deploy a weave-net pod:
kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')&env.IPALLOC_RANGE=10.50.0.0/16"

Container runtime

Check which container runtime is configured on kubelet

ps -aux | grep kubelet | grep --color container-runtime

The CNI binaries are located under /opt/cni/bin by default



General notes

https://kubernetes.io/docs/reference/kubectl/cheatsheet

